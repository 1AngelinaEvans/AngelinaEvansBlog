[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Angelina",
    "section": "",
    "text": "Angelina Evans is a senior at the University of Iowa. She is pursuing a major in Geography, as well as minors in Computer Science and Environmental Policy and Planning. Angelina is interested in how GIS and data science can be used to change or understand the world as we know it. In the Fall of 2023, she will begin the first year of the University of Iowa Undergraduate to Graduate (U2G) master’s degree program in Informatics with a focus in Geoinformatics. In the future, Angelina hopes to use her knowledge to identify problems and provide solutions to environmental and social issues related to climate change, disasters, pollution, health, and wellbeing."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "AngelinaEvansBlog",
    "section": "",
    "text": "Week 5 Blog\n\n\n\n\n\n\n\nWeek Five\n\n\n\n\n\n\n\n\n\n\n\nJun 14, 2023\n\n\nAngelina Evans\n\n\n\n\n\n\n  \n\n\n\n\nWeek 4 Blog\n\n\n\n\n\n\n\nWeek Four\n\n\n\n\n\n\n\n\n\n\n\nJun 6, 2023\n\n\nAngelina Evans\n\n\n\n\n\n\n  \n\n\n\n\nWeek 3 Blog\n\n\n\n\n\n\n\nWeek Three\n\n\n\n\n\n\n\n\n\n\n\nMay 31, 2023\n\n\nAngelina Evans\n\n\n\n\n\n\n  \n\n\n\n\nWeek 3 Wrap-Up\n\n\n\n\n\n\n\nWeek Three\n\n\nWeekly Wrap-ups\n\n\n\n\n\n\n\n\n\n\n\nMay 31, 2023\n\n\nAngelina Evans\n\n\n\n\n\n\n  \n\n\n\n\nWeek 1 Blog\n\n\n\n\n\n\n\nWeek One\n\n\n\n\n\n\n\n\n\n\n\nMay 23, 2023\n\n\nAngelina Evans\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 2 Blog\n\n\n\n\n\n\n\nWeek Two\n\n\n\n\n\n\n\n\n\n\n\nMay 23, 2023\n\n\nAngelina Evans\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Week1Blog/AngelinaWeek1.html",
    "href": "posts/Week1Blog/AngelinaWeek1.html",
    "title": "Week 1 Blog",
    "section": "",
    "text": "My First Week with DSPG\nDuring week one, I took a couple data camp courses to get more familiar with topics in R, Github, and web scraping in R.\nHere is an example of what I did in the Intro to R course. In the code below, there are two vectors: scores and comments. The scores represent the ratings of different movies and comments are opinions of viewers.\n\nscores <- c(4.6, 5, 4.8, 5, 4.2)\ncomments <- c(\"I would watch it again\", \"Amazing!\", \"I liked it\", \"One of the best movies\", \"Fascinating plot\")\n\nI can get the mean of the scores vector by writing:\n\nmean(scores)\n\n[1] 4.72\n\n\nSomething that was very new to me in R was matrices. I learned how to create them and how to reference specific rows and columns.\n\n# Vector with numerics from 1 up to 10\nmy_vector <- 1:10 \n\n# Matrix with numerics from 1 up to 9\nmy_matrix <- matrix(1:9, ncol = 3)\n\n# First 10 elements of the built-in data frame mtcars\nmy_df <- mtcars[1:10,]\n\n# Construct list with these different elements:\nmy_list <- list(my_vector,my_matrix,my_df)\n\nI plan to complete these courses: Intermediate R, Web Scraping in R, and AI Fundamentals.\n\n\n\nHere are two courses I want to complete soon. I also plan to take the AI Fundamentals course."
  },
  {
    "objectID": "posts/Week2Blog/AngelinaWeek2.html",
    "href": "posts/Week2Blog/AngelinaWeek2.html",
    "title": "Week 2 Blog",
    "section": "",
    "text": "TidyCensus\nKyle Walker’s tutorials introduce TidyCensus in R.\nHere is a bar graph showing median income for Story, Grundy, Chickasaw, and Buchanan counties. Also included is each estimate’s margin of error.\n\nlibrary(tidycensus)\nlibrary(ggplot2)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ lubridate 1.9.2     ✔ tibble    3.2.1\n✔ purrr     1.0.1     ✔ tidyr     1.3.0\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n\nmedian_income <- get_acs(\n  geography = \"county\",\n  variables = \"B19013_001\",\n  year = 2021\n)\n\nGetting data from the 2017-2021 5-year ACS\n\nwinvest_counties <- get_acs(\n  geography = 'county',\n  state = 'IA',\n  county = c('Story', 'Grundy', 'Chickasaw', 'Buchanan'),\n  variables = \"B19013_001\",\n  year = 2021,\n  survey = 'acs5' \n)\n\nGetting data from the 2017-2021 5-year ACS\n\nggplot(winvest_counties, aes(y = estimate, x = NAME)) + \n  ggtitle(\"Median Income\")+\n  geom_bar(stat=\"identity\", color = \"#3182bd\", fill=\"#9ecae1\")+labs(x=\"county name\",y=\"dollars\")+\n  scale_x_discrete(labels = function(x) str_remove(x, \" County, Iowa|, Iowa\"))+\n  geom_errorbar(aes(ymin = estimate - moe, ymax = estimate + moe),\n              width = 0.5, linewidth = 0.5)\n\n\n\n\n\n\nGitHub and Blogs\nWe also had a GitHub workshop this week to get familiar with GitHub actions. I created my own folder and ReadMe in the DSPG2023 Repo. We also created blogs using quarto/ RStudio.\n\n\nThings to Work On\nSetting up everything, especially the blog posts took a while and I had trouble with getting my changes to show up on GitHub. Thankfully I was able to get help and eventually got everything to show up correctly. I am looking forward to improving my blog pages and trying new things."
  },
  {
    "objectID": "posts/Week3Blog/AngelinaWeek3.html",
    "href": "posts/Week3Blog/AngelinaWeek3.html",
    "title": "Week 3 Blog",
    "section": "",
    "text": "This week was all about getting a better idea of the project’s direction and learning about ways to accomplish our project goals.\n\n\nOne idea for the project is that we can web scrape from housing assessor websites such as Iowa Assessors and Trulia. If we are able to successfully do so, we can use images of houses and other information to train our AI model. I focused on learning more about web scraping using R through the DataCamp Course called Web Scraping in R. Before that I also worked on the Intermediate R course to better understand what can be done with R. I have been trying to understand the web scraping process using DataCamp and other online tutorials.\nWe are also trying to gather images of houses and other data using a google API.\n\n\n\nExample of what I have been learning in DataCamp.\n\n\n\n\n\n\nAssessor page for buchanan county\n\n\n\n\n\nSomething that has been interesting this week is learning how we can use AI to help with our project. I am learning more about AI through the videos: Getting Started with Python Deep Learning for Beginners and Build a Deep CNN Image Classifier with ANY Images, and Gavin’s explaining.\n\n\n\nOne of my hopes from week 2 was that I would get better with editing and updating blogs. Now I feel a bit more comfortable with the blogs and things are working better for me.\nI also think that my group has a good idea of what we should get done. We were also able to come up with steps (ex. creating trials for our AI model that would be easier to troubleshoot).\n\n\n\nI definitely want to make more progress with web scraping because that is a key part of the project."
  },
  {
    "objectID": "posts/Week3TeamBlog/Week3TeamBlog.html",
    "href": "posts/Week3TeamBlog/Week3TeamBlog.html",
    "title": "Week 3 Wrap-Up",
    "section": "",
    "text": "This week, our team was able to spend time thinking and talking about:\n\nOur takeaways from last week’s client meeting\nProject goals\nMethods to use when working to reach project goals\nNew ideas for our project\n\n\n\n\nA rough plan for our project.\n\n\n\n\n\nWeb Scraping in R\nIntermediate R\n\n\n\n\nWe are currently trying to scrape data from housing assessor websites such as Iowa Assessors (data from Beacon and Vanguard) and Trulia. If we are able to successfully do so, images of houses and other information can be utilized to train our AI model(s).\nKailyn, Gavin and I (Angelina) focused on learning more about web scraping using R through the DataCamp Course called Web Scraping in R. We also began to follow steps for scraping the web using other tutorials.\n\n\n\nTo get an idea of how to use AI for the project, a sample model was trained to determine if a house has vegetation or no vegetation. About 750 images in total were used to train, validate and test the model.\nThe model was successfully trained and process did not take long.\n\n\n\n\n\n\n\n\n\n\n\nCreation of AI model was quicker than anticipated. We can spend more time into other aspects of the project (ex. data collection).\nKailyn was able to set up her blog!\nWe now have a better idea of the project’s methods and goals.\n\n\n\n\n\nStarting the web scraping process. When it comes to web scraping, we need to understand more than what the DataCamp course was teaching.\n\n\n\n\n\nGet a rough database running with housing info. We may be able to utilize a web interface that will allow us to more efficiently filter through houses.\nFamiliarize ourselves with web scraping.\nMeet with client(s) to show the project’s progress.\nFinish scraping data for Independence, Slater, New Hampton, Grundy Center, or other cities if needed.  \n\n\n\n\nHow many of you have some web scraping experience?"
  },
  {
    "objectID": "posts/Week4Blog/AngelinaWeek4.html",
    "href": "posts/Week4Blog/AngelinaWeek4.html",
    "title": "Week 4 Blog",
    "section": "",
    "text": "This week was all about data collection and cleaning. We started off by using Microsoft Excel to clean housing data collected from week three. I used several functions to parse data and create URLs to grab images of houses using Google API. Here are some things I learned:\n\n\nI worked on making changes to the the housing datasets for Grundy Center, Iowa. The format was different than what we needed, so using Excel’s Text-to-Columns I was able to fix the format.\nI also used a function to separate parcel ID and address:\n=TRIM(CLEAN(SUBSTITUTE(A1,CHAR(160),” “)))\nI also used text to column to separate names from addresses. I also needed to create urls from the addresses. To create the address uls in excel, I put + signs in between the address spaces by using this:\n=substitute(trim(cell),” “,”+“)\nTo combine the address with the first part of the URL, I use\n=cell&cell\n\n\n\n\n\n\n\nExcel file after fixing format and creating URL. We have done this process for Independence, New Hampton, and Slater, IA.\n\n\nWhen we click on one of the URL links (after adding an API Key to complete the URL), an image of the house appears. This is the house specified in the address.\nFor example, I take the URL from cell D2. The image is of a house from the URL of cell D2.\n\nWe hope to use these images to help train our AI models.\n\n\n\nWe had to re-prioritize scraping certain sources because some (Beacon and Vanguard) are protected while others (Zillow, Trulia) are not. We also learned that Trulia is owned by Zillow so our plans to scrape both changed as well. We now aim to scrape data from Zillow and Realtor.com, then find out if there are ways to scrape Beacon and Vanguard legally. The web scraping has been difficult this week, but as we make more progress we will be able to get more images and housing information from online.\nDuring week four I spent time finding ways to scrape the Zillow website. I tried to follow different tutorials and read more about web scraping. By Friday I was finally able to successfully scrape images of houses and addresses for Independence, IA on Zillow. This was the first time I was actually able to successfully scrape something.\nI got the base code from my one of the housing team members, Gavin.\nImages have now been downloaded on my personal computer.\n\n\n\n\nThe weeks are going by pretty fast, but week four felt like one long day because I mostly worked on trying to figure out how to web scrape websites. Figuring out how to web scrape was long, and oftentimes I was stuck. But I am glad I was able to make some progress and get help from others on my team.\n\n\n\nNext week I plan to get more comfortable with AI concepts so that I can begin building an AI model for the project."
  },
  {
    "objectID": "posts/Week5Blog/AngelinaWeek5.html",
    "href": "posts/Week5Blog/AngelinaWeek5.html",
    "title": "Week 5 Blog",
    "section": "",
    "text": "Preparing images for an AI model, training the AI model, and refreshing myself on mapping spatial data in R were my main focuses of week 5.\n\n\nThis week I worked on sorting images for the AI model that is used to determine if there is one house in the image or multiple houses. I sort the images into folders: one for images with one house, another for images with multiple houses. If there is not a house present, or if a building in the image is not a house, these images go in their own folders as well. Below are some examples of photos that belong to each category. Because I was the one sorting the model, I had to determine what qualified as an image with one house, multiple houses, no house or what was not a house. The images that had no house or not a house are not being used for the model, but it is helpful to sort these for other models that may be built or ones that could benefit from more images (ex. House present or not present model).\n\n\n\nAn image with only one house.\n\n\n\n\n\nAn image with multiple houses.\n\n\n\n\n\nAn image with no houses.\n\n\n\n\n\nThis is not a house.\n\n\nThere were many duplicate images in the photos, where pictures were taken of the same house. I did my best to remove duplicate photos. There were also many files that produced errors and an image of the house was not able to appear. I made sure to remove those as well so the AI does not consider these images in the training.\n\n\n\nImage not available.\n\n\n\n\n\nAfter I sorted all the Google images for the city of Slater, I am able to train the model that will be used to determine if there is one house in the photo or multiple houses. Below are screenshots showing the accuracy of the model, training outputs, and a test for the model. This is a work in progress, so I still have to work on better training the model. You can see that there is straight line, which is not supposed to be the case. I am working on identifying and fixing the issues with my model training.\n\n\n\nAccuracy of the model.\n\n\n\n\n\nTraining the model. 1 means there are multiple houses in the image, and 2 means there is only one house.\n\n\n\n\n\nMy test photo. When I feed the image to the model, the output should say: “Multiple houses”.\n\n\n\n\n\nIn this case, the model is correct!\n\n\nAnother issue with the model is that it has three classes instead of two. I am supposed to have 1s or 0s as output for training, but there are 1s and 2s. We are not fully sure why I have three classes instead of two, but will work more on this later. It is most likely a simple issue to resolve.\n\n\n\nFor the AI Housing project, we need to create some data visualizations that can help us and clients/users to better understand the project and the cities that may benefit from the project. To create maps in R, I need to refresh myself on mapping spatial data in R. I am doing the course called Visualizing Geospatial Data in R. I am also reading Chapter 5 Kyle Walker’s Tidy Census Book. The chapter is Census Geographic Data and Applications in R.\n\n\nI would like to geocode addresses for this project. I have 6,224 addresses that belong to the four Winvest cities: Slater, New Hampton, Grundy Center, Independence. I want to geocode and plot the addresses using QGIS. I used Google API key to geocode all the addresses. But I am only getting 953/6224 results. Right now I have coordinates for about 607 Slater addresses (all Slater addresses) and the other 300 something are for some of New Hampton’s addresses. QGIS creates an output file of all results where the address was not found.  When I open this output file, it is just blank.\nI have also tried geocoding and plotting only 100 addresses as a sample. It was able to geocode 92/100 addresses. But I was not able to see which ones failed and why. The output file of failed addresses was also blank. I am using MMQGIS. When I try to geocode, MMQGIS crashes but I am able to get output files. I updated MMQGIS but the same thing is happening.\n\n\n\n\n\n\nAddresses from Slater, IA.\n\n\n\nSome of the addresses for New Hampton, IA.\n\n\n\n\n\nA screenshot of a csv with some of the addresses that were sucessfully geocoded. The latitude and longitude columns are the result of geocoding with MMQGIS.\n\n\n\n\n\n\nOn Thursday we took a trip to the Iowa Technology and Geographic (ITAG) Council conference. I attended several presentations and visited vendor booths to learn more about the services they offer to state and local government, as well as other companies that assist local communities.\n\n\n\nThere were many vendors selling GIS and IT-related services. Some interns and I stopped by a ransomware vendor table.\n\n\n\n\n\nA presentation on hydrography and wetland modeling in Missouri.\n\n\n\n\n\nAnother presentation on ArcGIS field maps.\n\n\n\n\n\nNext week, I want to continue plotting addresses from areas we focused on when getting housing data. Hopefully my geocoding issues will be resolved. I also want to incorporate street basemaps to my maps and create better maps that we could use in our final project reports. Next week I also would like to use outputs provided by the AI models to display presence of houses with good or bad attributes (siding, gutter, etc).\nAs previously mentioned, I need to fix my AI model that determines whether a there is one house in an image or more than one."
  }
]