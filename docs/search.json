[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Angelina",
    "section": "",
    "text": "Angelina Evans is a senior at the University of Iowa. She is pursuing a major in Geography, as well as minors in Computer Science and Environmental Policy and Planning. Angelina is interested in how GIS and data science can be used to change or understand the world as we know it. In the future, Angelina hopes to use her knowledge to identify problems and provide solutions to environmental and social issues related to climate change, disasters, pollution, health, and wellbeing.\nThis blog is used to document work accomplished for the DSPG summer of 2023 program."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "AngelinaEvansBlog",
    "section": "",
    "text": "Guide to Visualizing House Quality\n\n\n\n\n\n\n\nMapping\n\n\n\n\n\n\n\n\n\n\n\nJun 29, 2023\n\n\nAngelina Evans\n\n\n\n\n\n\n  \n\n\n\n\nWeek 6 Team Wrap-Up\n\n\n\n\n\n\n\nWeek Six\n\n\nWeekly Wrap-ups\n\n\n\n\n\n\n\n\n\n\n\nJun 23, 2023\n\n\nAngelina Evans\n\n\n\n\n\n\n  \n\n\n\n\nWeek 6 Blog\n\n\n\n\n\n\n\nWeek Six\n\n\n\n\n\n\n\n\n\n\n\nJun 23, 2023\n\n\nAngelina Evans\n\n\n\n\n\n\n  \n\n\n\n\nWeek 6 Team Wrap-Up\n\n\n\n\n\n\n\nWeek Six\n\n\nWeekly Wrap-ups\n\n\n\n\n\n\n\n\n\n\n\nJun 23, 2023\n\n\nAngelina Evans\n\n\n\n\n\n\n  \n\n\n\n\nWeek 5 Blog\n\n\n\n\n\n\n\nWeek Five\n\n\n\n\n\n\n\n\n\n\n\nJun 16, 2023\n\n\nAngelina Evans\n\n\n\n\n\n\n  \n\n\n\n\nWeek 4 Blog\n\n\n\n\n\n\n\nWeek Four\n\n\n\n\n\n\n\n\n\n\n\nJun 6, 2023\n\n\nAngelina Evans\n\n\n\n\n\n\n  \n\n\n\n\nWeek 3 Blog\n\n\n\n\n\n\n\nWeek Three\n\n\n\n\n\n\n\n\n\n\n\nMay 31, 2023\n\n\nAngelina Evans\n\n\n\n\n\n\n  \n\n\n\n\nWeek 3 Wrap-Up\n\n\n\n\n\n\n\nWeek Three\n\n\nWeekly Wrap-ups\n\n\n\n\n\n\n\n\n\n\n\nMay 31, 2023\n\n\nAngelina Evans\n\n\n\n\n\n\n  \n\n\n\n\nWeek 1 Blog\n\n\n\n\n\n\n\nWeek One\n\n\n\n\n\n\n\n\n\n\n\nMay 23, 2023\n\n\nAngelina Evans\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 2 Blog\n\n\n\n\n\n\n\nWeek Two\n\n\n\n\n\n\n\n\n\n\n\nMay 23, 2023\n\n\nAngelina Evans\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/House Quality Dashboards/HouseQualityDashboars.html",
    "href": "posts/House Quality Dashboards/HouseQualityDashboars.html",
    "title": "Week 6 Team Wrap-Up",
    "section": "",
    "text": "New Hampton records in the Fulcrum app.\n\n\nOn Monday and Tuesday of week six, the DSPG group traveled to Grundy Center, New Hampton and Independence, Iowa. In groups of 3-4 people, we walked around residential areas and observed the condition of houses and lots. The Fulcrum app was used to record observations. We assigned good, fair and poor ratings to characteristics such as: Roof, gutter, landscape, siding, and sidewalk to the house.\n\n\n\n\n\nWhile in residential neighborhoods, we also gave general impressions of blocks by observing presence and quality of: neighborhood sidewalks, street lights, way-finding signs, storm drainage systems, curb cuts and street trees.\n\n\n\nOne of the goals of the project has been to provide demographic profiles of three cities: Independence, New Hampton and Grundy Center. We want to show characteristics related to economics, housing and population, and how these characteristics have been changing through the years or how they are expected to change in the future. Several graphs were made in R.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSo far, all models needed have been implemented! The accuracy will continue to improve the more they are trained. Now what is needed is a way to get the model outputs into a format that is useful for us. We need to write the outputs to a csv file. The script below is used to write to a CSV file in the correct attribute column based on the address being evaluated.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOne of our clients wishes to see something related to the project in the form of maps. One idea is to use maps to display well-trained AI model ratings for landscapes, siding, gutters etc. This will allow us and our clients to visualize where houses in good condition are versus lower condition.\nTo visualize addresses for the WinVEST cities and Slater, first we needed to geocode them (get a latitude and longitude for each address). Base code was from storybench.\n\n\n\n\n\n\n\n\n\n\nAddresses are plotted in QGIS using the coordinates. There are some addresses that are not in the correct place (highlighted below). This is something to look into next week.\n\n\n\n\n\n\n\nA sample map was created in which gutter quality for houses in the city are shown.\n\n\n\nLegend is still in progress. The darkest orange is “poor”, lighter orange is “fair”, and the blue is “good”.\n\n\n\n\n\n\nOn Thursday we met online with Tara Brueggeman, an Assessor for Mason City, and Erin Mullenix, one of our clients. We learned many things from this meeting. These are our main takeaways:\n\nThe data provided by Beacon and Vanguard is public but we are not able to web scrape. However, city and county assessor offices can get some data from Vanguard:\n\nA city/county assessor office has access to the Vanguard assessment data management system. Technically, they can run some customized reports on the data using SQL but many offices, especially in smaller IA communities, don’t know how to get data from the system using SQL.\n\nWe learned about the appraisal process Tara and her team follow:\n\nAppraisal starts with the use of a blueprint. Find information on number of bedrooms and bathrooms, square footage, and details on basement.\nAppraiser goes to the house to measure and inspect in-person. They used to go inside houses more often, but now homeowners are less comfortable with that. It does help to take a look at the inside of houses, especially because the inside does not always match blueprint. However, the condition of the exterior is usually a good indicator of the condition of the interior.\n\nTara will provide us with Iowa Real Property appraisal manual, which is is given to assessors. This may be useful for us because we could get better ideas of how different characteristics of a house are assessed.\nHow they aim to minimize bias when assessing?\n\nNo assessing when hungover, having a bad day, or if it’s raining outside.\nThe records are reviewed by another person before being finalized.\n\n\n\n\n\nAs we get down to the last few weeks, we are beginning to finish up major project tasks.\n\n\n\nIt’s taking a while to figure out how to write the model’s ratings to a csv file both effectively and efficiently.\n\n\n\n\nMeet with Erin Olsen-Douglas and Erin Mullenix (our clients) to give an update on what we have accomplished since last meeting. We plan to come prepared with several questions.\nCreate a teaser video and slides for our project.\nFinish script that will allow us to have the model’s outputs in a csv file.\nUse files to visualize the quality of houses.\nFurther train the models using new images of houses in Des Moines.\n\n\n\n\nWhat was the weirdest or most shocking conversation you had with a resident while collecting data?"
  },
  {
    "objectID": "posts/MappingHouseData/MappingHouseQuality.html",
    "href": "posts/MappingHouseData/MappingHouseQuality.html",
    "title": "Guide to Visualizing House Quality",
    "section": "",
    "text": "In this guide, I will be sharing the steps I took to create geographic visualizations in Tableau related to housing quality. To demonstrate the process, I will be using data from the city of Ogden, Iowa. Another AI Housing team member, Gavin, created a guide on evaluating cities using Ogden as an example. In hopes that the data will appear familiar, I will be using the resulting data from his city evaluation guide.\n\n\n\nSteps for address collection and dataset cleaning are shown in the address collection and cleaning section of Gavin’s guide. It is very important that you have these things available in the dataset:\n\nHouse number\nStreet name\nCity\nState\nQuality of house characteristics you wish to use in your geographic visualizations (ex. AI outputs for siding, vegetation, gutter).\nIf you followed the steps in the address collection and cleaning section of Gavin’s guide, your dataset might have some columns that look like this:\n\n\nI suggest creating a new column that has combines contents from address, city, and state. I auto-populated the city column with the city name (Ogden) and the state column with the state name (Iowa). In a new column and new empty cell, I used the & to combine the contents of three columns. My function is:\n=Cell&“,”&Cell&“,”Cell\n=A2&“,”&B2&“,”&C2\nThen I auto-populated the rest of the cells to utilize the same function.\n\n\n\n\n\n\n\n\nUsing the addresses provided from the dataset, I acquired coordinates that can be used to map the address locations. There are several ways to geocode addresses. If you are familiar with ArcGIS or QGIS, you can use that but I had major issues with QGIS geocoding. I used the MMQGIS plugin and had multiple problems. First, it continued to crash and second, I could never successfully geocode all the addresses I needed to. Many of my addresses were missing and other team members had the same issue when using MMQGIS. So I would advise against starting with QGIS, unless it has worked well for you in the past.\nYou can also use python or R. I geocoded addresses in RStudio. I followed the approach provided by StoryBench. I made a few changes to the code from StoryBench. The code and steps are shown below.\n\nStart by installing the ggmap package (no need to install ggplot2 unless you later want to plot addresses in R).\nLoad in the csv file (Ogden database).\nCreate data frames.\nRegister a Google API key. Put the key in the “key goes here” spot.\nFor loop: for each address, you receive a latitude and longitude. They should go in their respected columns (lat and long). Line 26 has the column name full_address. Change this to the name of the column that has your full addresses.\nWrite the contents of origAddress to a new file (in this case, ogden_geocoded2.csv). Save the new file.\n\n\n\nOnce you have geocoded the addresses, the latitude and longitude columns should be filled. Remove the NAs from the other columns.\n\n\n\n\nThere are many different ways you can map these addresses. If you have access to ArcGIS pro, you can use that. I also like using QGIS, an open source software. In this guide I will be using Tableau because this is what I used for my final visualizations. We decided to use Tableau because we needed to create other visualizations (ex. graphs) with the data provided in the csv file.\n\n\nAs a student, you can register to get Tableau Desktop free for one year. I was having installation issues, so I use Tableau Public, which is free to anyone. You can use either for these visualizations, but my steps will be for Tableau Public. The desktop version is very similar so you should be able to follow along if using Tableau Desktop.\nIf you are using Tableau Public, be sure to save you work by publishing.\n\n\n\nOnce you click “Create”, you should be prompted to upload data from your computer. Select the csv file you wish to use.\n\n\n\nWatching this video by TutorialsPoint helped me get started on mapping in Tableau.\nAfter uploaded the data, click the sheet tab on the bottom. I renamed my sheet to ogden_siding. From the “Data” tab on the left side of the screen, find Lon and Lat and drag both to columns and rows at the top of the screen. Right click on Lon and Lat and make sure that Dimension is checked, not attribute or measure. When this is done, you can click the “Show Me” button on the upper right part of the screen. Click on the map that is circled in the image. You should then be able to see addresses on a map. Click size to shrink or grow the points. To show a particular housing attribute, drag the desired variable to the “Color” button that is circled under the marks section.\nIn my example, when the mouse hovers over a point, it will give the condition of the house’s siding.\nAt the top of the screen, go to Map -> Basemap -> Streets to add a streetmap.\n\n\n\n\nOnce you are satisfied with how your map looks, create a new dashboard at the bottom of the screen. Load in the sheet by dragging the sheet you want to use. In the dashboard, make edits to legend, title, and map scale.\n\n\n\nRight-click on legend to find options to change legend colors, attribute names (edit alias), and more.\n\n\nYou may wish to continue to experiment with point size, borders and halos once you see how it looks in the dashboard.\nYou can add more than one sheet to a dashboard. My example uses one sheet, the Ogden siding sheet. In my dashboard, I right-clicked the null attribute and clicked “exclude”. Now my map only shows houses with either chipped paint, poor siding or good siding.\n\n\n\nOnce you are satisfied, download the dashboard. I downloaded my dashboard as an image. From this map, I am able to see that the siding AI model determined that many of the houses in Ogden have poor siding."
  },
  {
    "objectID": "posts/MappingHouseData/MappingHouseQuality.html#conclusion",
    "href": "posts/MappingHouseData/MappingHouseQuality.html#conclusion",
    "title": "Guide to Visualizing House Quality",
    "section": "Conclusion",
    "text": "Conclusion\nThis concludes the guide to mapping house quality. Hopefully this has given information that is helpful when preparing and visualizing spatial data. Again, if you’re using Tableau Public, don’t forget save work by publishing. My work has been lost a couple times because I forgot to publish."
  },
  {
    "objectID": "posts/Week1Blog/AngelinaWeek1.html",
    "href": "posts/Week1Blog/AngelinaWeek1.html",
    "title": "Week 1 Blog",
    "section": "",
    "text": "My First Week with DSPG\nDuring week one, I took a couple data camp courses to get more familiar with topics in R, Github, and web scraping in R.\nHere is an example of what I did in the Intro to R course. In the code below, there are two vectors: scores and comments. The scores represent the ratings of different movies and comments are opinions of viewers.\n\nscores <- c(4.6, 5, 4.8, 5, 4.2)\ncomments <- c(\"I would watch it again\", \"Amazing!\", \"I liked it\", \"One of the best movies\", \"Fascinating plot\")\n\nI can get the mean of the scores vector by writing:\n\nmean(scores)\n\n[1] 4.72\n\n\nSomething that was very new to me in R was matrices. I learned how to create them and how to reference specific rows and columns.\n\n# Vector with numerics from 1 up to 10\nmy_vector <- 1:10 \n\n# Matrix with numerics from 1 up to 9\nmy_matrix <- matrix(1:9, ncol = 3)\n\n# First 10 elements of the built-in data frame mtcars\nmy_df <- mtcars[1:10,]\n\n# Construct list with these different elements:\nmy_list <- list(my_vector,my_matrix,my_df)\n\nI plan to complete these courses: Intermediate R, Web Scraping in R, and AI Fundamentals.\n\n\n\nHere are two courses I want to complete soon. I also plan to take the AI Fundamentals course."
  },
  {
    "objectID": "posts/Week2Blog/AngelinaWeek2.html",
    "href": "posts/Week2Blog/AngelinaWeek2.html",
    "title": "Week 2 Blog",
    "section": "",
    "text": "TidyCensus\nKyle Walker’s tutorials introduce TidyCensus in R.\nHere is a bar graph showing median income for Story, Grundy, Chickasaw, and Buchanan counties. Also included is each estimate’s margin of error.\n\nlibrary(tidycensus)\nlibrary(ggplot2)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ lubridate 1.9.2     ✔ tibble    3.2.1\n✔ purrr     1.0.1     ✔ tidyr     1.3.0\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n\nmedian_income <- get_acs(\n  geography = \"county\",\n  variables = \"B19013_001\",\n  year = 2021\n)\n\nGetting data from the 2017-2021 5-year ACS\n\nwinvest_counties <- get_acs(\n  geography = 'county',\n  state = 'IA',\n  county = c('Story', 'Grundy', 'Chickasaw', 'Buchanan'),\n  variables = \"B19013_001\",\n  year = 2021,\n  survey = 'acs5' \n)\n\nGetting data from the 2017-2021 5-year ACS\n\nggplot(winvest_counties, aes(y = estimate, x = NAME)) + \n  ggtitle(\"Median Income\")+\n  geom_bar(stat=\"identity\", color = \"#3182bd\", fill=\"#9ecae1\")+labs(x=\"county name\",y=\"dollars\")+\n  scale_x_discrete(labels = function(x) str_remove(x, \" County, Iowa|, Iowa\"))+\n  geom_errorbar(aes(ymin = estimate - moe, ymax = estimate + moe),\n              width = 0.5, linewidth = 0.5)\n\n\n\n\n\n\nGitHub and Blogs\nWe also had a GitHub workshop this week to get familiar with GitHub actions. I created my own folder and ReadMe in the DSPG2023 Repo. We also created blogs using quarto/ RStudio.\n\n\nThings to Work On\nSetting up everything, especially the blog posts took a while and I had trouble with getting my changes to show up on GitHub. Thankfully I was able to get help and eventually got everything to show up correctly. I am looking forward to improving my blog pages and trying new things."
  },
  {
    "objectID": "posts/Week3Blog/AngelinaWeek3.html",
    "href": "posts/Week3Blog/AngelinaWeek3.html",
    "title": "Week 3 Blog",
    "section": "",
    "text": "This week was all about getting a better idea of the project’s direction and learning about ways to accomplish our project goals.\n\n\nOne idea for the project is that we can web scrape from housing assessor websites such as Iowa Assessors and Trulia. If we are able to successfully do so, we can use images of houses and other information to train our AI model. I focused on learning more about web scraping using R through the DataCamp Course called Web Scraping in R. Before that I also worked on the Intermediate R course to better understand what can be done with R. I have been trying to understand the web scraping process using DataCamp and other online tutorials.\nWe are also trying to gather images of houses and other data using a google API.\n\n\n\nExample of what I have been learning in DataCamp.\n\n\n\n\n\n\nAssessor page for buchanan county\n\n\n\n\n\nSomething that has been interesting this week is learning how we can use AI to help with our project. I am learning more about AI through the videos: Getting Started with Python Deep Learning for Beginners and Build a Deep CNN Image Classifier with ANY Images, and Gavin’s explaining.\n\n\n\nOne of my hopes from week 2 was that I would get better with editing and updating blogs. Now I feel a bit more comfortable with the blogs and things are working better for me.\nI also think that my group has a good idea of what we should get done. We were also able to come up with steps (ex. creating trials for our AI model that would be easier to troubleshoot).\n\n\n\nI definitely want to make more progress with web scraping because that is a key part of the project."
  },
  {
    "objectID": "posts/Week3TeamBlog/Week3TeamBlog.html",
    "href": "posts/Week3TeamBlog/Week3TeamBlog.html",
    "title": "Week 3 Wrap-Up",
    "section": "",
    "text": "This week, our team was able to spend time thinking and talking about:\n\nOur takeaways from last week’s client meeting\nProject goals\nMethods to use when working to reach project goals\nNew ideas for our project\n\n\n\n\nA rough plan for our project.\n\n\n\n\n\nWeb Scraping in R\nIntermediate R\n\n\n\n\nWe are currently trying to scrape data from housing assessor websites such as Iowa Assessors (data from Beacon and Vanguard) and Trulia. If we are able to successfully do so, images of houses and other information can be utilized to train our AI model(s).\nKailyn, Gavin and I (Angelina) focused on learning more about web scraping using R through the DataCamp Course called Web Scraping in R. We also began to follow steps for scraping the web using other tutorials.\n\n\n\nTo get an idea of how to use AI for the project, a sample model was trained to determine if a house has vegetation or no vegetation. About 750 images in total were used to train, validate and test the model.\nThe model was successfully trained and process did not take long.\n\n\n\n\n\n\n\n\n\n\n\nCreation of AI model was quicker than anticipated. We can spend more time into other aspects of the project (ex. data collection).\nKailyn was able to set up her blog!\nWe now have a better idea of the project’s methods and goals.\n\n\n\n\n\nStarting the web scraping process. When it comes to web scraping, we need to understand more than what the DataCamp course was teaching.\n\n\n\n\n\nGet a rough database running with housing info. We may be able to utilize a web interface that will allow us to more efficiently filter through houses.\nFamiliarize ourselves with web scraping.\nMeet with client(s) to show the project’s progress.\nFinish scraping data for Independence, Slater, New Hampton, Grundy Center, or other cities if needed.  \n\n\n\n\nHow many of you have some web scraping experience?"
  },
  {
    "objectID": "posts/Week4Blog/AngelinaWeek4.html",
    "href": "posts/Week4Blog/AngelinaWeek4.html",
    "title": "Week 4 Blog",
    "section": "",
    "text": "This week was all about data collection and cleaning. We started off by using Microsoft Excel to clean housing data collected from week three. I used several functions to parse data and create URLs to grab images of houses using Google API. Here are some things I learned:\n\n\nOn Tuesday, we visited the city of Slater, IA to familiarize ourselves with assessing houses for the WINVEST project.\n\n\n\nWe practiced using the Fulcrum app on houses and main street businesses.\n\n\n\n\n\nI worked on making changes to the the housing datasets for Grundy Center, Iowa. The format was different than what we needed, so using Excel’s Text-to-Columns I was able to fix the format.\nI also used a function to separate parcel ID and address:\n=TRIM(CLEAN(SUBSTITUTE(A1,CHAR(160),” “)))\nI also used text to column to separate names from addresses. I also needed to create urls from the addresses. To create the address uls in excel, I put + signs in between the address spaces by using this:\n=substitute(trim(cell),” “,”+“)\nTo combine the address with the first part of the URL, I use\n=cell&cell\n\n\n\n\nExcel file after fixing format and creating URL. We have done this process for Independence, New Hampton, and Slater, IA.\n\n\nWhen we click on one of the URL links (after adding an API Key to complete the URL), an image of the house appears. This is the house specified in the address.\nFor example, I take the URL from cell D2. The image is of a house from the URL of cell D2.\n\nWe hope to use these images to help train our AI models.\n\n\n\nWe had to re-prioritize scraping certain sources because some (Beacon and Vanguard) are protected while others (Zillow, Trulia) are not. We also learned that Trulia is owned by Zillow so our plans to scrape both changed as well. We now aim to scrape data from Zillow and Realtor.com, then find out if there are ways to scrape Beacon and Vanguard legally. The web scraping has been difficult this week, but as we make more progress we will be able to get more images and housing information from online.\nDuring week four I spent time finding ways to scrape the Zillow website. I tried to follow different tutorials and read more about web scraping. By Friday I was finally able to successfully scrape images of houses and addresses for Independence, IA on Zillow. This was the first time I was actually able to successfully scrape something.\nI got the base code from my one of the housing team members, Gavin.\nImages have now been downloaded on my personal computer.\n\n\n\n\nThe weeks are going by pretty fast, but week four felt like one long day because I mostly worked on trying to figure out how to web scrape websites. Figuring out how to web scrape was long, and oftentimes I was stuck. But I am glad I was able to make some progress and get help from others on my team.\n\n\n\nNext week I plan to get more comfortable with AI concepts so that I can begin building an AI model for the project."
  },
  {
    "objectID": "posts/Week5Blog/AngelinaWeek5.html",
    "href": "posts/Week5Blog/AngelinaWeek5.html",
    "title": "Week 5 Blog",
    "section": "",
    "text": "Preparing images for an AI model, training the AI model, and refreshing myself on mapping spatial data in R were my main focuses of week 5.\n\n\nThis week I worked on sorting images for the AI model that is used to determine if there is one house in the image or multiple houses. I sort the images into folders: one for images with one house, another for images with multiple houses. If there is not a house present, or if a building in the image is not a house, these images go in their own folders as well. Below are some examples of photos that belong to each category. Because I was the one sorting the model, I had to determine what qualified as an image with one house, multiple houses, no house or what was not a house. The images that had no house or not a house are not being used for the model, but it is helpful to sort these for other models that may be built or ones that could benefit from more images (ex. House present or not present model).\n\n\n\nAn image with only one house.\n\n\n\n\n\nAn image with multiple houses.\n\n\n\n\n\nAn image with no houses.\n\n\n\n\n\nThis is not a house.\n\n\nThere were many duplicate images in the photos, where pictures were taken of the same house. I did my best to remove duplicate photos. There were also many files that produced errors and an image of the house was not able to appear. I made sure to remove those as well so the AI does not consider these images in the training.\n\n\n\nImage not available.\n\n\n\n\n\nAfter I sorted all the Google images for the city of Slater, I am able to train the model that will be used to determine if there is one house in the photo or multiple houses. Below are screenshots showing the accuracy of the model, training outputs, and a test for the model. This is a work in progress, so I still have to work on better training the model. You can see that there is straight line, which is not supposed to be the case. I am working on identifying and fixing the issues with my model training.\n\n\n\nAccuracy of the model.\n\n\n\n\n\nTraining the model. 1 means there are multiple houses in the image, and 2 means there is only one house.\n\n\n\n\n\nMy test photo. When I feed the image to the model, the output should say: “Multiple houses”.\n\n\n\n\n\nIn this case, the model is correct!\n\n\nAnother issue with the model is that it has three classes instead of two. I am supposed to have 1s or 0s as output for training, but there are 1s and 2s. We are not fully sure why I have three classes instead of two, but will work more on this later. It is most likely a simple issue to resolve.\n\n\n\nFor the AI Housing project, we need to create some data visualizations that can help us and clients/users to better understand the project and the cities that may benefit from the project. To create maps in R, I need to refresh myself on mapping spatial data in R. I am doing the course called Visualizing Geospatial Data in R. I am also reading Chapter 5 Kyle Walker’s Tidy Census Book. The chapter is Census Geographic Data and Applications in R.\n\n\nI would like to geocode addresses for this project. I have 6,224 addresses that belong to the four Winvest cities: Slater, New Hampton, Grundy Center, Independence. I want to geocode and plot the addresses using QGIS. I used Google API key to geocode all the addresses. But I am only getting 953/6224 results. Right now I have coordinates for about 607 Slater addresses (all Slater addresses) and the other 300 something are for some of New Hampton’s addresses. QGIS creates an output file of all results where the address was not found.  When I open this output file, it is just blank.\nI have also tried geocoding and plotting only 100 addresses as a sample. It was able to geocode 92/100 addresses. But I was not able to see which ones failed and why. The output file of failed addresses was also blank. I am using MMQGIS. When I try to geocode, MMQGIS crashes but I am able to get output files. I updated MMQGIS but the same thing is happening.\n\n\n\n\n\n\nAddresses from Slater, IA.\n\n\n\nSome of the addresses for New Hampton, IA.\n\n\n\n\n\nA screenshot of a csv with some of the addresses that were sucessfully geocoded. The latitude and longitude columns are the result of geocoding with MMQGIS.\n\n\n\n\n\n\nOn Thursday we took a trip to the Iowa Technology and Geographic (ITAG) Council conference. I attended several presentations and visited vendor booths to learn more about the services they offer to state and local government, as well as other companies that assist local communities.\n\n\n\nThere were many vendors selling GIS and IT-related services. Some interns and I stopped by a ransomware vendor table.\n\n\n\n\n\nA presentation on hydrography and wetland modeling in Missouri.\n\n\n\n\n\nAnother presentation on ArcGIS field maps.\n\n\n\n\n\nNext week, I want to continue plotting addresses from areas we focused on when getting housing data. Hopefully my geocoding issues will be resolved. I also want to incorporate street basemaps to my maps and create better maps that we could use in our final project reports. Next week I also would like to use outputs provided by the AI models to display presence of houses with good or bad attributes (siding, gutter, etc).\nAs previously mentioned, I need to fix my AI model that determines whether a there is one house in an image or more than one."
  },
  {
    "objectID": "posts/Week6Blog/AngelinaWeek6.html",
    "href": "posts/Week6Blog/AngelinaWeek6.html",
    "title": "Week 6 Blog",
    "section": "",
    "text": "The first two days of week six were spent collecting data for neighborhoods in Grundy Center, New Hampton and Independence, Iowa. With others from the DSPG group, I used the Fulcrum app to record observations about houses and areas in residential neighborhoods. I assigned good, fair and poor ratings to characteristics such as the roof, gutter, landscape, siding, and sidewalk to the house. I also gave general impressions of midblocks. I observed presence of street lights, signs, and noted the condition of storm drainage systems, sidewalks, and curb cuts. At first it took me a while to do these things, but by the second city, I felt more comfortable with the tasks.\nAfter those two days, my team was able to have conversations on what we thought about the data collection. We noticed that each person had their own qualifications to what was considered a poor attribute versus a fair and good attribute. We thought that using this information for the purpose of training AI models would potentially mislead what we want the AI to believe about a house. This is why some of us may need to go back and re-sort the photos for the model training just to be sure the information isn’t conflicting with what we want the AI to understand about qualifications.\n\n\n\n\n\n\nRecords in New Hampton on the Fullcrum app.\n\n\n\n\n\nI am continuing to work with addresses of the four cities to plot them. My goal is to use maps to display well-trained AI model ratings for landscapes, siding, gutters etc. This will allow us and our clients to visualize where houses in good condition are versus lower condition.\nTo visualize addresses for the WinVEST cities and Slater, first I used R to geocode them. Base code was from storybench. Here are screenshots of the code. It took about twenty minutes for around 6,600 addresses to be geocoded using this script.\nI thought this part would not take me very long, but it did take me a while. I kept running into issues. Even though I had the base code available to me, some parts just didn’t work so I had to re-write new lines and figure out what worked best.\nWhen I though the addresses had been geocoded successfully, I tried to plot them in QGIS but only one point showed. It took time before I realized that there were indeed about 6,600 point but they all had the same coordinate. I had to retrace my steps and find out why this happened. Finally, I was able to get the right coordinates.\n\n\n\n\n\n\n\n\n\n\nI then plotted the addresses using QGIS. I used OpenStreetMap as a basemap. There are some addresses that are not in the correct place (highlighted below). For example, one point is in Linn County where it should be somewhere in Independence. This is something that I will look into more next week. I may remove stray points if needed.\n\n\n\n\n\n\n\nI made a sample map that shows gutter quality for houses in Slater. Hopefully next week I can make maps that show the real condition of other characteristics. I made a new column in excel that randomly has either “Good”, “Fair”, or “Poor” written in each cell. With that field, I was able to to create a map showing good, fair and poor houses in Slater. Though the labels were completely random, it was nice to see what it would look like to show the conditions of the houses spatially. I am currently having trouble with the map legend, so hopefully by the time the csv files are ready with attributes given by AI, I will have enough knowledge to complete several more maps.\n\n\n\nLegend is still in progress. The darkest orange is “poor”, lighter orange is “fair”, and the blue is “good”."
  },
  {
    "objectID": "posts/Week6TeamBlog/Week6TeamBlog.html",
    "href": "posts/Week6TeamBlog/Week6TeamBlog.html",
    "title": "Week 6 Team Wrap-Up",
    "section": "",
    "text": "New Hampton records in the Fulcrum app.\n\n\nOn Monday and Tuesday of week six, the DSPG group traveled to Grundy Center, New Hampton and Independence, Iowa. In groups of 3-4 people, we walked around residential areas and observed the condition of houses and lots. The Fulcrum app was used to record observations. We assigned good, fair and poor ratings to characteristics such as: Roof, gutter, landscape, siding, and sidewalk to the house.\n\n\n\n\n\nWhile in residential neighborhoods, we also gave general impressions of blocks by observing presence and quality of: neighborhood sidewalks, street lights, way-finding signs, storm drainage systems, curb cuts and street trees.\n\n\n\nOne of the goals of the project has been to provide demographic profiles of three cities: Independence, New Hampton and Grundy Center. We want to show characteristics related to economics, housing and population, and how these characteristics have been changing through the years or how they are expected to change in the future. Several graphs were made in R.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSo far, all models needed have been implemented! The accuracy will continue to improve the more they are trained. Now what is needed is a way to get the model outputs into a format that is useful for us. We need to write the outputs to a csv file. The script below is used to write to a CSV file in the correct attribute column based on the address being evaluated.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOne of our clients wishes to see something related to the project in the form of maps. One idea is to use maps to display well-trained AI model ratings for landscapes, siding, gutters etc. This will allow us and our clients to visualize where houses in good condition are versus lower condition.\nTo visualize addresses for the WinVEST cities and Slater, first we needed to geocode them (get a latitude and longitude for each address). Base code was from storybench.\n\n\n\n\n\n\n\n\n\n\nAddresses are plotted in QGIS using the coordinates. There are some addresses that are not in the correct place (highlighted below). This is something to look into next week.\n\n\n\n\n\n\n\nA sample map was created in which gutter quality for houses in the city are shown.\n\n\n\nLegend is still in progress. The darkest orange is “poor”, lighter orange is “fair”, and the blue is “good”.\n\n\n\n\n\n\nOn Thursday we met online with Tara Brueggeman, an Assessor for Mason City, and Erin Mullenix, one of our clients. We learned many things from this meeting. These are our main takeaways:\n\nThe data provided by Beacon and Vanguard is public but we are not able to web scrape. However, city and county assessor offices can get some data from Vanguard:\n\nA city/county assessor office has access to the Vanguard assessment data management system. Technically, they can run some customized reports on the data using SQL but many offices, especially in smaller IA communities, don’t know how to get data from the system using SQL.\n\nWe learned about the appraisal process Tara and her team follow:\n\nAppraisal starts with the use of a blueprint. Find information on number of bedrooms and bathrooms, square footage, and details on basement.\nAppraiser goes to the house to measure and inspect in-person. They used to go inside houses more often, but now homeowners are less comfortable with that. It does help to take a look at the inside of houses, especially because the inside does not always match blueprint. However, the condition of the exterior is usually a good indicator of the condition of the interior.\n\nTara will provide us with Iowa Real Property appraisal manual, which is is given to assessors. This may be useful for us because we could get better ideas of how different characteristics of a house are assessed.\nHow they aim to minimize bias when assessing?\n\nNo assessing when hungover, having a bad day, or if it’s raining outside.\nThe records are reviewed by another person before being finalized.\n\n\n\n\n\nAs we get down to the last few weeks, we are beginning to finish up major project tasks.\n\n\n\nIt’s taking a while to figure out how to write the model’s ratings to a csv file both effectively and efficiently.\n\n\n\n\nMeet with Erin Olsen-Douglas and Erin Mullenix (our clients) to give an update on what we have accomplished since last meeting. We plan to come prepared with several questions.\nCreate a teaser video and slides for our project.\nFinish script that will allow us to have the model’s outputs in a csv file.\nUse files to visualize the quality of houses.\nFurther train the models using new images of houses in Des Moines.\n\n\n\n\nWhat was the weirdest or most shocking conversation you had with a resident while collecting data?"
  }
]