[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About Angelina\n\nAngelina Evans is a senior at the University of Iowa. She is pursuing a major in Geography, as well as minors in Computer Science and Environmental Policy and Planning. Angelina is interested in how GIS and data science can be used to change or understand the world as we know it. In the Fall of 2023, she will begin the first year of the University of Iowa Undergraduate to Graduate (U2G) master’s degree program in Informatics with a focus in Geoinformatics. In the future, Angelina hopes to use her knowledge to identify problems and provide solutions to environmental and social issues related to climate change, disasters, pollution, health, and wellbeing."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "AngelinaEvansBlog",
    "section": "",
    "text": "Week 3 Blog\n\n\n\n\n\n\n\nWeek Three\n\n\n\n\n\n\n\n\n\n\n\nMay 31, 2023\n\n\nAngelina Evans\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 3 Blog\n\n\n\n\n\n\n\nWeek Three\n\n\n\n\n\n\n\n\n\n\n\nMay 31, 2023\n\n\nAngelina Evans\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTeam Blog for Week 3\n\n\n\n\n\n\n\nWeek 3\n\n\nHousing Team\n\n\n\n\n\n\n\n\n\n\n\nMay 31, 2023\n\n\nAngelina Evans\n\n\n\n\n\n\n  \n\n\n\n\nWeek 1 Blog\n\n\n\n\n\n\n\nWeek One\n\n\n\n\n\n\n\n\n\n\n\nMay 23, 2023\n\n\nAngelina Evans\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 2 Blog\n\n\n\n\n\n\n\nWeek Two\n\n\n\n\n\n\n\n\n\n\n\nMay 23, 2023\n\n\nAngelina Evans\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/New Blog/new.html",
    "href": "posts/New Blog/new.html",
    "title": "Week 3 Blog",
    "section": "",
    "text": "This week, our group was able to time thinking and talking about:\n\nOur takeaways from last week’s client meeting\nProject goals\nMethods to use when working to reach project goals\nNew ideas to add into our project\nRecurring concerns\n\nAddressing the points above as a group allowed me to get a better idea of our project goals and the directions we should take.\n\n\n\nOne hope for the project is that we can web scrape from housing assessor websites such as Iowa Assessors and Trulia. If we are able to successfully do so, we can use images of houses and other information to train our AI model. I focused on learning more about web scraping using R through the DataCamp Course called Web Scraping in R. Before that I also worked on the Intermediate R course to better understand what can be done with R.\nI also began to start the web scraping process using R.\n\n\n\nOne of my hopes from week 2 was that I would get better with editing and updating blogs. Now I feel a bit more comfortable with the blogs and things are working better for me.\nI also think that my group has a good idea of what we should get done. We were also able to come up with steps (ex. creating trials for our AI model that would be easier to troubleshoot).\n\n\n\nI definitely want to make more progress with web scraping because that is a key part of the project."
  },
  {
    "objectID": "posts/Week1Blog/AngelinaWeek1.html",
    "href": "posts/Week1Blog/AngelinaWeek1.html",
    "title": "Week 1 Blog",
    "section": "",
    "text": "My First Week with DSPG\nDuring week one, I took a couple data camp courses to get more familiar with topics in R, Github, and web scraping in R.\nHere is an example of what I did in the Intro to R course. In the code below, there are two vectors: scores and comments. The scores represent the ratings of different movies and comments are opinions of viewers.\n\nscores <- c(4.6, 5, 4.8, 5, 4.2)\ncomments <- c(\"I would watch it again\", \"Amazing!\", \"I liked it\", \"One of the best movies\", \"Fascinating plot\")\n\nI can get the mean of the scores vector by writing:\n\nmean(scores)\n\n[1] 4.72\n\n\nSomething that was very new to me in R was matrices. I learned how to create them and how to reference specific rows and columns.\n\n# Vector with numerics from 1 up to 10\nmy_vector <- 1:10 \n\n# Matrix with numerics from 1 up to 9\nmy_matrix <- matrix(1:9, ncol = 3)\n\n# First 10 elements of the built-in data frame mtcars\nmy_df <- mtcars[1:10,]\n\n# Construct list with these different elements:\nmy_list <- list(my_vector,my_matrix,my_df)\n\nI plan to complete these courses: Intermediate R, Web Scraping in R, and AI Fundamentals.\n\n\n\nHere are two courses I want to complete soon. I also plan to take the AI Fundamentals course."
  },
  {
    "objectID": "posts/Week2Blog/AngelinaWeek2.html",
    "href": "posts/Week2Blog/AngelinaWeek2.html",
    "title": "Week 2 Blog",
    "section": "",
    "text": "TidyCensus\nKyle Walker’s tutorials introduce TidyCensus in R.\nHere is a bar graph showing median income for Story, Grundy, Chickasaw, and Buchanan counties. Also included is each estimate’s margin of error.\n\nlibrary(tidycensus)\nlibrary(ggplot2)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ lubridate 1.9.2     ✔ tibble    3.2.1\n✔ purrr     1.0.1     ✔ tidyr     1.3.0\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n\nmedian_income <- get_acs(\n  geography = \"county\",\n  variables = \"B19013_001\",\n  year = 2021\n)\n\nGetting data from the 2017-2021 5-year ACS\n\nwinvest_counties <- get_acs(\n  geography = 'county',\n  state = 'IA',\n  county = c('Story', 'Grundy', 'Chickasaw', 'Buchanan'),\n  variables = \"B19013_001\",\n  year = 2021,\n  survey = 'acs5' \n)\n\nGetting data from the 2017-2021 5-year ACS\n\nggplot(winvest_counties, aes(y = estimate, x = NAME)) + \n  ggtitle(\"Median Income\")+\n  geom_bar(stat=\"identity\", color = \"#3182bd\", fill=\"#9ecae1\")+labs(x=\"county name\",y=\"dollars\")+\n  scale_x_discrete(labels = function(x) str_remove(x, \" County, Iowa|, Iowa\"))+\n  geom_errorbar(aes(ymin = estimate - moe, ymax = estimate + moe),\n              width = 0.5, linewidth = 0.5)\n\n\n\n\n\n\nGitHub and Blogs\nWe also had a GitHub workshop this week to get familiar with GitHub actions. I created my own folder and ReadMe in the DSPG2023 Repo. We also created blogs using quarto/ RStudio.\n\n\nThings to Work On\nSetting up everything, especially the blog posts took a while and I had trouble with getting my changes to show up on GitHub. Thankfully I was able to get help and eventually got everything to show up correctly. I am looking forward to improving my blog pages and trying new things."
  },
  {
    "objectID": "posts/Week3Blog/AngelinaWeek3.html",
    "href": "posts/Week3Blog/AngelinaWeek3.html",
    "title": "Week 3 Blog",
    "section": "",
    "text": "This week, our group was able to time thinking and talking about:\n\nOur takeaways from last week’s client meeting\nProject goals\nMethods to use when working to reach project goals\nNew ideas to add into our project\nRecurring concerns\n\nAddressing the points above as a group allowed me to get a better idea of our project goals and the directions we should take.\n\n\n\nOne hope for the project is that we can web scrape from housing assessor websites such as Iowa Assessors and Trulia. If we are able to successfully do so, we can use images of houses and other information to train our AI model. I focused on learning more about web scraping using R through the DataCamp Course called Web Scraping in R. Before that I also worked on the Intermediate R course to better understand what can be done with R.\nI also began to start the web scraping process using R.\n\n\n\nOne of my hopes from week 2 was that I would get better with editing and updating blogs. Now I feel a bit more comfortable with the blogs and things are working better for me.\nI also think that my group has a good idea of what we should get done. We were also able to come up with steps (ex. creating trials for our AI model that would be easier to troubleshoot).\n\n\n\nI definitely want to make more progress with web scraping because that is a key part of the project."
  },
  {
    "objectID": "posts/Week3TeamBlog/Week3TeamBlog.html",
    "href": "posts/Week3TeamBlog/Week3TeamBlog.html",
    "title": "Team Blog for Week 3",
    "section": "",
    "text": "Housing Team Week 3 Recap\n## Project Progress\nThis week, our group was able to time thinking and talking about:\n- Our takeaways from last week’s client meeting\n- Project goals\n- Methods to use when working to reach project goals\n- Ways to work together on parts of the project\n- New ideas to add into our project\n- Recurring concerns\nOur group began to come up with a plan to use AI models as well as data we get from the housing assessor websites and Google API to rate houses.\n\nWeb Scraping\n\n\n\n\n\nAI Modeling\n##good"
  }
]